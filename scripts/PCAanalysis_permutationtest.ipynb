{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcium traces exploration - simple commands in pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib import cm\n",
    "from sklearn.model_selection import KFold, cross_validate, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation tests: first principal component (airpuff, sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script includes:\n",
    "    -Permutation tests to compare different time windows around points of interest using first principal \n",
    "    component for airpuff & non-airpuff sessions (using sessions & trials as datapoints)\n",
    "    -Permutation tests to compare airpuff vs non-airpuff sessions for each time window using first principal\n",
    "    component (using sessions & trials as datapoints)\n",
    "\n",
    "---------\n",
    "Python 3.8.5\n",
    "\n",
    "Dependencies:\n",
    "    -numpy 1.19.2\n",
    "    -pandas 1.1.3\n",
    "    -scipy 1.6.3\n",
    "    -matplotlib 3.3.2\n",
    "    -seaborn 0.11.0\n",
    "    -sklearn 0.0\n",
    "    -csv, os, pickle, random, copy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_folder_name = \"/Users/felicia/Documents/CFIN/232/232_traces/\"\n",
    "session_list = ['04','06','07','08','10','11','12','13'] # 232\n",
    "#session_list = ['06','07','08','09','10','11','12','13'] # 233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate empty lists\n",
    "first_var_start = []\n",
    "first_var_stim_onset = []\n",
    "first_var_resp = []\n",
    "first_var_punish = []\n",
    "first_var_reward = []\n",
    "\n",
    "# for all airpuff sessions:\n",
    "for session in range(14,20):\n",
    "\n",
    "    # LOAD CSV CALCIUM TRACES\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # LOAD BEHAVIOURAL DATA\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data into trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "    airpuff = 2\n",
    "\n",
    "    # start of trial\n",
    "    variances_start = []\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial)\n",
    "    var_start = np.mean(variances_start, axis=0)\n",
    "\n",
    "    # stimulus onset\n",
    "    variances_stim_onset = []\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial)\n",
    "    var_stim_onset = np.mean(variances_stim_onset, axis=0)\n",
    "\n",
    "    # response\n",
    "    variances_resp = []\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial)\n",
    "    var_resp = np.mean(variances_resp, axis=0)\n",
    "\n",
    "    # punishment\n",
    "    variances_punish = []\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_airpuff_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate) + airpuff\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial)\n",
    "    var_punish = np.mean(variances_punish, axis=0)\n",
    "    \n",
    "    # reward\n",
    "    variances_reward = []\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial)\n",
    "    var_reward = np.mean(variances_reward, axis=0)\n",
    "\n",
    "    first_var_start.append(var_start[0])\n",
    "    first_var_stim_onset.append(var_stim_onset[0])\n",
    "    first_var_resp.append(var_resp[0])\n",
    "    first_var_punish.append(var_punish[0])\n",
    "    first_var_reward.append(var_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation test: pvalue = 0.985\n",
      "parametric t-test: Ttest_indResult(statistic=-0.31067269546489856, pvalue=0.7624282505406907)\n"
     ]
    }
   ],
   "source": [
    "# change variable1 and variable2 to compute permutation and t-tests between them:\n",
    "# first_var_start  first_var_stim_onset  first_var_resp  first_var_punish  first_var_reward\n",
    "variable1 = first_var_punish\n",
    "variable2 = first_var_reward\n",
    "\n",
    "# Compute the ground truth absolute difference Δτ between τ₁ and τ₂ from your two variables:\n",
    "gT = np.abs(np.average(variable1) - np.average(variable2))\n",
    "\n",
    "# Pool your variables into one single distribution:\n",
    "pV = list(variable1) + list(variable2)\n",
    "\n",
    "# Randomly sample (also called bootstrapping) without replacement two distributions \n",
    "# with the size equal to the original distributions from this pooled distribution \n",
    "# to compute the absolute difference RΔτ of your metric between your two permuted samples:\n",
    "# and repeat this p times:\n",
    "\n",
    "# Copy pooled distribution:\n",
    "pS = copy.copy(pV)\n",
    "# Initialize permutation:\n",
    "pD = []\n",
    "# Define p (number of permutations):\n",
    "p=1000\n",
    "# Permutation loop:\n",
    "for i in range(0,p):\n",
    "  # Shuffle the data:\n",
    "    random.shuffle(pS)\n",
    "    # Compute permuted absolute difference of your two sampled distributions and store it in pD:\n",
    "    pD.append(np.abs(np.average(pS[0:int(len(pS)/2)]) - np.average(pS[int(len(pS)/2):])))\n",
    "    \n",
    "# Finally, the proportion of permuted differences higher than your ground truth difference \n",
    "# is your significance value:\n",
    "p_val = len(np.where(pD>=gT)[0])/p\n",
    "print(\"permutation test: pvalue =\", p_val)\n",
    "# Compare with parametric t-test:\n",
    "print(\"parametric t-test:\", scipy.stats.stats.ttest_ind(variable1, variable2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation tests: first principal component (airpuff, trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initiate empty lists\n",
    "variances_start = []\n",
    "variances_stim_onset = []\n",
    "variances_resp = []\n",
    "variances_punish = []\n",
    "variances_reward = []\n",
    "tot_trials = 0\n",
    "\n",
    "# for all airpuff sessions:\n",
    "for session in range(14,20):\n",
    "\n",
    "    # LOAD CSV CALCIUM TRACES\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # LOAD BEHAVIOURAL DATA\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    #tot_trials = tot_trials + n_trials # total number of trials across all sessions\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "    tot_trials = tot_trials + n_trials # total number of trials across all sessions\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    X = {}\n",
    "\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "    airpuff = 2\n",
    "\n",
    "    # start of trial\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial[0])\n",
    "\n",
    "    # stimulus onset\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial[0])\n",
    "\n",
    "    # response\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial[0])\n",
    "\n",
    "    # punishment\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_airpuff_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate) + airpuff\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial[0])\n",
    "    \n",
    "    # reward\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation test: pvalue = 0.009\n",
      "parametric t-test: Ttest_indResult(statistic=array([2.13176585, 4.03497078, 2.51066894, 0.91667998, 2.54266201,\n",
      "       1.40233269, 1.40233269]), pvalue=array([3.42712779e-02, 7.82353350e-05, 1.28586449e-02, 3.60436884e-01,\n",
      "       1.17737222e-02, 1.62397768e-01, 1.62397768e-01]))\n"
     ]
    }
   ],
   "source": [
    "# change variable1 and variable2 to compute permutation and t-tests between them:\n",
    "# variances_start  variances_stim_onset  variances_resp  variances_punish  variances_reward\n",
    "variable1 = variances_punish\n",
    "variable2 = variances_reward\n",
    "\n",
    "# Compute the ground truth absolute difference Δτ between τ₁ and τ₂ from your two variables:\n",
    "gT = np.abs(np.average(variable1) - np.average(variable2))\n",
    "\n",
    "# Pool your variables into one single distribution:\n",
    "pV = list(variable1) + list(variable2)\n",
    "\n",
    "# Randomly sample (also called bootstrapping) without replacement two distributions \n",
    "# with the size equal to the original distributions from this pooled distribution \n",
    "# to compute the absolute difference RΔτ of your metric between your two permuted samples:\n",
    "# and repeat this p times:\n",
    "\n",
    "# Copy pooled distribution:\n",
    "pS = copy.copy(pV)\n",
    "# Initialize permutation:\n",
    "pD = []\n",
    "# Define p (number of permutations):\n",
    "p=1000\n",
    "# Permutation loop:\n",
    "for i in range(0,p):\n",
    "  # Shuffle the data:\n",
    "    random.shuffle(pS)\n",
    "    # Compute permuted absolute difference of your two sampled distributions and store it in pD:\n",
    "    pD.append(np.abs(np.average(pS[0:int(len(pS)/2)]) - np.average(pS[int(len(pS)/2):])))\n",
    "    \n",
    "    \n",
    "# multiple testing correction?\n",
    "    \n",
    "# Finally, the proportion of permuted differences higher than your ground truth difference \n",
    "# is your significance value:\n",
    "p_val = len(np.where(pD>=gT)[0])/p\n",
    "print(\"permutation test: pvalue =\", p_val)\n",
    "# Compare with parametric t-test:\n",
    "print(\"parametric t-test:\", scipy.stats.stats.ttest_ind(variable1, variable2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation tests: first principal component (no airpuff, sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initiate empty lists\n",
    "first_var_start = []\n",
    "first_var_stim_onset = []\n",
    "first_var_resp = []\n",
    "first_var_punish = []\n",
    "first_var_reward = []\n",
    "\n",
    "# for all non-airpuff sessions:\n",
    "for session in session_list:\n",
    "\n",
    "    # load csv calcium traces\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "    for i in range(1,len(time_array)):\n",
    "        time_sampling[i-1] = time_array[i]-time_array[i-1]\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # load the behavioral data\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #trials_finish_reward = trial_info['trial_info'][0,0][6].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data into trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "\n",
    "    # start of trial\n",
    "    variances_start = []\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial)\n",
    "    var_start = np.mean(variances_start, axis=0)\n",
    "\n",
    "    # stimulus onset\n",
    "    variances_stim_onset = []\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial)\n",
    "    var_stim_onset = np.mean(variances_stim_onset, axis=0)\n",
    "\n",
    "    # response\n",
    "    variances_resp = []\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial)\n",
    "    var_resp = np.mean(variances_resp, axis=0)\n",
    "\n",
    "    # punishment\n",
    "    variances_punish = []\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial)\n",
    "    var_punish = np.mean(variances_punish, axis=0)\n",
    "    \n",
    "    # reward\n",
    "    variances_reward = []\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial)\n",
    "    var_reward = np.mean(variances_reward, axis=0)\n",
    "\n",
    "    first_var_start.append(var_start[0])\n",
    "    first_var_stim_onset.append(var_stim_onset[0])\n",
    "    first_var_resp.append(var_resp[0])\n",
    "    first_var_punish.append(var_punish[0])\n",
    "    first_var_reward.append(var_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation test: pvalue = 0.374\n",
      "parametric t-test: Ttest_indResult(statistic=-0.8993176119907691, pvalue=0.38369122971726255)\n"
     ]
    }
   ],
   "source": [
    "# change variable1 and variable2 to compute permutation and t-tests between them:\n",
    "# first_var_start  first_var_stim_onset  first_var_resp  first_var_punish  first_var_reward\n",
    "variable1 = first_var_punish\n",
    "variable2 = first_var_reward\n",
    "\n",
    "# Compute the ground truth absolute difference Δτ between τ₁ and τ₂ from your two variables:\n",
    "gT = np.abs(np.average(variable1) - np.average(variable2))\n",
    "\n",
    "# Pool your variables into one single distribution:\n",
    "pV = list(variable1) + list(variable2)\n",
    "\n",
    "# Randomly sample (also called bootstrapping) without replacement two distributions \n",
    "# with the size equal to the original distributions from this pooled distribution \n",
    "# to compute the absolute difference RΔτ of your metric between your two permuted samples:\n",
    "# and repeat this p times:\n",
    "\n",
    "# Copy pooled distribution:\n",
    "pS = copy.copy(pV)\n",
    "# Initialize permutation:\n",
    "pD = []\n",
    "# Define p (number of permutations):\n",
    "p=1000\n",
    "# Permutation loop:\n",
    "for i in range(0,p):\n",
    "  # Shuffle the data:\n",
    "    random.shuffle(pS)\n",
    "    # Compute permuted absolute difference of your two sampled distributions and store it in pD:\n",
    "    pD.append(np.abs(np.average(pS[0:int(len(pS)/2)]) - np.average(pS[int(len(pS)/2):])))\n",
    "    \n",
    "# Finally, the proportion of permuted differences higher than your ground truth difference \n",
    "# is your significance value:\n",
    "p_val = len(np.where(pD>=gT)[0])/p\n",
    "print(\"permutation test: pvalue =\", p_val)\n",
    "# Compare with parametric t-test:\n",
    "print(\"parametric t-test:\", scipy.stats.stats.ttest_ind(variable1, variable2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation tests: first principal component (no airpuff, trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate empty lists\n",
    "variances_start = []\n",
    "variances_stim_onset = []\n",
    "variances_resp = []\n",
    "variances_punish = []\n",
    "variances_reward = []\n",
    "tot_trials = 0\n",
    "\n",
    "# for all non-airpuff sessions:\n",
    "for session in session_list:\n",
    "\n",
    "    # load csv calcium traces\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "    for i in range(1,len(time_array)):\n",
    "        time_sampling[i-1] = time_array[i]-time_array[i-1]\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # load the behavioral data\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #trials_finish_reward = trial_info['trial_info'][0,0][6].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    #tot_trials = tot_trials + n_trials # total number of trials across sessions\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "    tot_trials = tot_trials + n_trials # total number of trials across sessions\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data by trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "\n",
    "    # start of trial\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial[0])\n",
    "\n",
    "    # stimulus onset\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial[0])\n",
    "\n",
    "    # response\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial[0])\n",
    "\n",
    "    # punishment\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial[0])\n",
    "    \n",
    "    # reward\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation test: pvalue = 0.218\n",
      "parametric t-test: Ttest_indResult(statistic=-1.201537438182991, pvalue=0.2297387329783271)\n"
     ]
    }
   ],
   "source": [
    "# change variable1 and variable2 to compute permutation and t-tests between them:\n",
    "# variances_start  variances_stim_onset  variances_resp  variances_punish  variances_reward\n",
    "variable1 = variances_punish\n",
    "variable2 = variances_reward\n",
    "\n",
    "# Compute the ground truth absolute difference Δτ between τ₁ and τ₂ from your two variables:\n",
    "gT = np.abs(np.average(variable1) - np.average(variable2))\n",
    "\n",
    "# Pool your variables into one single distribution:\n",
    "pV = list(variable1) + list(variable2)\n",
    "\n",
    "# Randomly sample (also called bootstrapping) without replacement two distributions \n",
    "# with the size equal to the original distributions from this pooled distribution \n",
    "# to compute the absolute difference RΔτ of your metric between your two permuted samples:\n",
    "# and repeat this p times:\n",
    "\n",
    "# Copy pooled distribution:\n",
    "pS = copy.copy(pV)\n",
    "# Initialize permutation:\n",
    "pD = []\n",
    "# Define p (number of permutations):\n",
    "p=1000\n",
    "# Permutation loop:\n",
    "for i in range(0,p):\n",
    "  # Shuffle the data:\n",
    "    random.shuffle(pS)\n",
    "    # Compute permuted absolute difference of your two sampled distributions and store it in pD:\n",
    "    pD.append(np.abs(np.average(pS[0:int(len(pS)/2)]) - np.average(pS[int(len(pS)/2):])))\n",
    "    \n",
    "# Finally, the proportion of permuted differences higher than your ground truth difference \n",
    "# is your significance value:\n",
    "p_val = len(np.where(pD>=gT)[0])/p\n",
    "print(\"permutation test: pvalue =\", p_val)\n",
    "# Compare with parametric t-test:\n",
    "print(\"parametric t-test:\", scipy.stats.stats.ttest_ind(variable1, variable2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation tests: airpuff vs no airpuff (sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### airpuff ###\n",
    "# initiate empty lists\n",
    "first_var_start_ap = []\n",
    "first_var_stim_onset_ap = []\n",
    "first_var_resp_ap = []\n",
    "first_var_punish_ap = []\n",
    "first_var_reward_ap = []\n",
    "\n",
    "# for all airpuff sessions:\n",
    "for session in range(14,20):\n",
    "\n",
    "    # LOAD CSV CALCIUM TRACES\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # LOAD BEHAVIOURAL DATA\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data by trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "    airpuff = 2\n",
    "\n",
    "    # start of trial\n",
    "    variances_start = []\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial)\n",
    "    var_start = np.mean(variances_start, axis=0)\n",
    "\n",
    "    # stimulus onset\n",
    "    variances_stim_onset = []\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial)\n",
    "    var_stim_onset = np.mean(variances_stim_onset, axis=0)\n",
    "\n",
    "    # response\n",
    "    variances_resp = []\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial)\n",
    "    var_resp = np.mean(variances_resp, axis=0)\n",
    "\n",
    "    # punishment\n",
    "    variances_punish = []\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_airpuff_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate) + airpuff\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial)\n",
    "    var_punish = np.mean(variances_punish, axis=0)\n",
    "    \n",
    "    # reward\n",
    "    variances_reward = []\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial)\n",
    "    var_reward = np.mean(variances_reward, axis=0)\n",
    "\n",
    "    first_var_start_ap.append(var_start[0])\n",
    "    first_var_stim_onset_ap.append(var_stim_onset[0])\n",
    "    first_var_resp_ap.append(var_resp[0])\n",
    "    first_var_punish_ap.append(var_punish[0])\n",
    "    first_var_reward_ap.append(var_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no airpuff ###\n",
    "# initiate empty lists\n",
    "first_var_start = []\n",
    "first_var_stim_onset = []\n",
    "first_var_resp = []\n",
    "first_var_punish = []\n",
    "first_var_reward = []\n",
    "\n",
    "# for all non-airpuff sessions:\n",
    "for session in session_list:\n",
    "\n",
    "    # load csv calcium traces\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "    for i in range(1,len(time_array)):\n",
    "        time_sampling[i-1] = time_array[i]-time_array[i-1]\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # load the behavioral data\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #trials_finish_reward = trial_info['trial_info'][0,0][6].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data by trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "\n",
    "    # start of trial\n",
    "    variances_start = []\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial)\n",
    "    var_start = np.mean(variances_start, axis=0)\n",
    "\n",
    "    # stimulus onset\n",
    "    variances_stim_onset = []\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial)\n",
    "    var_stim_onset = np.mean(variances_stim_onset, axis=0)\n",
    "\n",
    "    # response\n",
    "    variances_resp = []\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial)\n",
    "    var_resp = np.mean(variances_resp, axis=0)\n",
    "\n",
    "    # punishment\n",
    "    variances_punish = []\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial)\n",
    "    var_punish = np.mean(variances_punish, axis=0)\n",
    "    \n",
    "    # reward\n",
    "    variances_reward = []\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial)\n",
    "    var_reward = np.mean(variances_reward, axis=0)\n",
    "\n",
    "    first_var_start.append(var_start[0])\n",
    "    first_var_stim_onset.append(var_stim_onset[0])\n",
    "    first_var_resp.append(var_resp[0])\n",
    "    first_var_punish.append(var_punish[0])\n",
    "    first_var_reward.append(var_reward[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation test: pvalue = 0.46\n",
      "parametric t-test: Ttest_indResult(statistic=-0.7800516640808547, pvalue=0.45047298680038195)\n"
     ]
    }
   ],
   "source": [
    "# change variable1 and variable2 to compute permutation and t-tests between them:\n",
    "# airpuff: first_var_start_ap  first_var_stim_onset_ap  first_var_resp_ap  first_var_punish_ap  first_var_reward_ap\n",
    "# no airpuff: first_var_start  first_var_stim_onset  first_var_resp  first_var_punish  first_var_reward\n",
    "variable1 = first_var_reward_ap\n",
    "variable2 = first_var_reward\n",
    "\n",
    "# Compute the ground truth absolute difference Δτ between τ₁ and τ₂ from your two variables:\n",
    "gT = np.abs(np.average(variable1) - np.average(variable2))\n",
    "\n",
    "# Pool your variables into one single distribution:\n",
    "pV = list(variable1) + list(variable2)\n",
    "\n",
    "# Randomly sample (also called bootstrapping) without replacement two distributions \n",
    "# with the size equal to the original distributions from this pooled distribution \n",
    "# to compute the absolute difference RΔτ of your metric between your two permuted samples:\n",
    "# and repeat this p times:\n",
    "\n",
    "# Copy pooled distribution:\n",
    "pS = copy.copy(pV)\n",
    "# Initialize permutation:\n",
    "pD = []\n",
    "# Define p (number of permutations):\n",
    "p=1000\n",
    "# Permutation loop:\n",
    "for i in range(0,p):\n",
    "  # Shuffle the data:\n",
    "    random.shuffle(pS)\n",
    "    # Compute permuted absolute difference of your two sampled distributions and store it in pD:\n",
    "    pD.append(np.abs(np.average(pS[0:int(len(pS)/2)]) - np.average(pS[int(len(pS)/2):])))\n",
    "    \n",
    "# Finally, the proportion of permuted differences higher than your ground truth difference \n",
    "# is your significance value:\n",
    "p_val = len(np.where(pD>=gT)[0])/p\n",
    "print(\"permutation test: pvalue =\", p_val)\n",
    "# Compare with parametric t-test:\n",
    "print(\"parametric t-test:\", scipy.stats.stats.ttest_ind(variable1, variable2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation tests: airpuff vs no airpuff (trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### no airpuff ###\n",
    "# initiate empty lists\n",
    "variances_start = []\n",
    "variances_stim_onset = []\n",
    "variances_resp = []\n",
    "variances_punish = []\n",
    "variances_reward = []\n",
    "tot_trials = 0\n",
    "\n",
    "# for all non-airpuff sessions:\n",
    "for session in session_list:\n",
    "\n",
    "    # load csv calcium traces\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "    for i in range(1,len(time_array)):\n",
    "        time_sampling[i-1] = time_array[i]-time_array[i-1]\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # load the behavioral data\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #trials_finish_reward = trial_info['trial_info'][0,0][6].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    #tot_trials = tot_trials + n_trials # total number of trials across sessions\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "    tot_trials = tot_trials + n_trials # total number of trials across sessions\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data by trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "\n",
    "    # start of trial\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_start.append(var_trial[0])\n",
    "\n",
    "    # stimulus onset\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_stim_onset.append(var_trial[0])\n",
    "\n",
    "    # response\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_resp.append(var_trial[0])\n",
    "\n",
    "    # punishment\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_punish.append(var_trial[0])\n",
    "    \n",
    "    # reward\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_reward.append(var_trial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### airpuff ###\n",
    "# initiate empty lists\n",
    "variances_ap_start = []\n",
    "variances_ap_stim_onset = []\n",
    "variances_ap_resp = []\n",
    "variances_ap_punish = []\n",
    "variances_ap_reward = []\n",
    "tot_trials = 0\n",
    "\n",
    "# for all airpuff sessions:\n",
    "for session in range(14,20):\n",
    "\n",
    "    # LOAD CSV CALCIUM TRACES\n",
    "    folder_name = \"Calcium_Traces/\" \n",
    "    filename = \"2021-05-{0}.csv\".format(session)\n",
    "    df = pd.read_csv(os.path.join(animal_folder_name,folder_name, filename))\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    # create time array\n",
    "    df_times = df[' '][1:-1]\n",
    "    df_num_time = pd.to_numeric(df_times, downcast='float')\n",
    "    time_array = df_num_time.to_numpy()\n",
    "    tot_timepoints = len(time_array)\n",
    "\n",
    "    # check time sampling - i.e. if every data point is taken at the same \"distance\" in time from the next - or previous\n",
    "    time_sampling = np.zeros(shape=(len(time_array)-1,))\n",
    "\n",
    "    #12420 timepoints sampled at 100msec. means that every 10 of those timepoints is 1 sec\n",
    "    sampling_rate = 0.1\n",
    "    tot_secs = int(tot_timepoints * sampling_rate)\n",
    "\n",
    "    #create channels array\n",
    "    n_channels = n_columns-1\n",
    "    channels_array = np.zeros(shape=(tot_timepoints, n_channels)) # initiate empty array\n",
    "    # fill empty channels array\n",
    "    for i in range(n_channels):\n",
    "        col_name = df.columns[i+1]\n",
    "        df_channel = df[col_name][1:-1]\n",
    "        df_num_channel = pd.to_numeric(df_channel, downcast='float')\n",
    "        channels_array[:,i] = df_num_channel.to_numpy()\n",
    "\n",
    "    # LOAD BEHAVIOURAL DATA\n",
    "    #folder_name = \"trial_info\"\n",
    "    #bhv_info_filename = \"trial_info_2021-05-{0}.mat\".format(session)\n",
    "    #trial_info = scipy.io.loadmat(os.path.join(animal_folder_name,folder_name, bhv_info_filename))\n",
    "\n",
    "    # extract information from trial_info: in matlab this is a struct with the fields:\n",
    "    #trials_start_time = trial_info['trial_info'][0,0][0].ravel()\n",
    "    #trials_stimulus_on = trial_info['trial_info'][0,0][1].ravel()\n",
    "    #trials_response = trial_info['trial_info'][0,0][2].ravel()\n",
    "    #trials_is_right_lick = trial_info['trial_info'][0,0][3].ravel()\n",
    "    #trials_is_reward = trial_info['trial_info'][0,0][4].ravel()\n",
    "    #trials_end_time = trial_info['trial_info'][0,0][5].ravel()\n",
    "    #n_trials = len(trials_end_time) # number of trials\n",
    "    #tot_trials = tot_trials + n_trials # total number of trials across sessions\n",
    "    \n",
    "    # load behavioural data (.pkl file)\n",
    "    input_filename = \"trial_info/trial_info_2021-05-{0}.pkl\".format(session)\n",
    "    with open(os.path.join(animal_folder_name,input_filename), 'rb') as fp:\n",
    "         trial_info = pickle.load(fp)\n",
    "\n",
    "    # extract information for trial_info: in PYTHON\n",
    "    trials_start_time = np.array(trial_info['start_time'])\n",
    "    trials_stimulus_on = np.array(trial_info['stimulus_on'])\n",
    "    trials_response = np.array(trial_info['response'])\n",
    "    trials_is_right_lick = np.array(trial_info['is_right_lick'])\n",
    "    trials_is_reward = np.array(trial_info['is_reward'])\n",
    "    trials_end_time = np.array(trial_info['end_trial'])\n",
    "    n_trials = len(trials_end_time)\n",
    "    tot_trials = tot_trials + n_trials # total number of trials across sessions\n",
    "\n",
    "    # 500msec between start and stimulus onset\n",
    "    new_start = [] # initiate empty list\n",
    "    new_end = [] # initiate empty list\n",
    "    new_start.append(trials_stimulus_on[0]-0.5) # append first start\n",
    "    new_end.append(trials_stimulus_on[1]-0.5) # append first end\n",
    "    for i in range(1,n_trials-1): # append start & end times for each trial (except last)\n",
    "        new_start.append(trials_stimulus_on[i]-0.5)\n",
    "        new_end.append(trials_stimulus_on[i+1]-0.5)\n",
    "    new_start.append(trials_stimulus_on[n_trials-1]-0.5) # append last start\n",
    "    new_end.append(trials_end_time[n_trials-1]) # append last end\n",
    "    new_start = np.array(new_start) # turn list to array\n",
    "    new_end = np.array(new_end)\n",
    "    n_trials = len(new_end)\n",
    "    \n",
    "    # where was the stimulus? we don't have this info. But it can be reconstructed \n",
    "    stimulus_reconstruction = np.zeros(shape=(n_trials,))\n",
    "    right_choice = np.where(trials_is_reward==1)[0]\n",
    "    stimulus_reconstruction[right_choice] = trials_is_right_lick[right_choice]\n",
    "    wrong_choice = np.where(trials_is_reward==0)[0]\n",
    "    stimulus_reconstruction[wrong_choice] = 1 - trials_is_right_lick[wrong_choice]\n",
    "\n",
    "    # split traces data by trials\n",
    "    X = {}\n",
    "    for i in range(n_trials-1):\n",
    "        lower_bound = np.min(np.where(time_array>=new_start[i]))\n",
    "        upper_bound = np.max(np.where(time_array<=new_end[i]))\n",
    "        X[i] = channels_array[lower_bound:upper_bound,:]\n",
    "        \n",
    "        \n",
    "    ###### across all trials for this session\n",
    "    n_components = 7\n",
    "    start_timepoint = 0\n",
    "    n_after_start = 7\n",
    "    n_before_stim = 2\n",
    "    n_after_stim = 5\n",
    "    n_before_resp = 5\n",
    "    n_after_resp = 2\n",
    "    n_before_pun = 2\n",
    "    n_after_pun = 5\n",
    "    n_before_rew = 2\n",
    "    n_after_rew = 5\n",
    "    time_window = n_before_stim+n_after_stim\n",
    "    airpuff = 2\n",
    "\n",
    "    # start of trial\n",
    "    for idx in X:\n",
    "        x = X[idx][start_timepoint:n_after_start]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_ap_start.append(var_trial[0])\n",
    "\n",
    "    # stimulus onset\n",
    "    for idx in X:\n",
    "        stim_timepoint = int((trials_stimulus_on[idx] - new_start[idx]) /sampling_rate)\n",
    "        x = X[idx][stim_timepoint-n_before_stim:stim_timepoint + n_after_stim]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_ap_stim_onset.append(var_trial[0])\n",
    "\n",
    "    # response\n",
    "    for idx in X:\n",
    "        resp_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate)\n",
    "        x = X[idx][resp_timepoint - n_before_resp:resp_timepoint+n_after_resp]\n",
    "        if len(x)>6: # make sure that time window is not too small\n",
    "            covar_matrix = PCA(n_components=n_components)\n",
    "            covar_matrix.fit(x)\n",
    "            variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "            var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_ap_resp.append(var_trial[0])\n",
    "\n",
    "    # punishment\n",
    "    for idx in X:\n",
    "        # only punishment trials\n",
    "        if trials_is_reward[idx]==0:\n",
    "            # timepoint for response\n",
    "            resp_airpuff_timepoint = int((trials_response[idx] - new_start[idx]) / sampling_rate) + airpuff\n",
    "            x = X[idx][resp_timepoint-n_before_pun:resp_timepoint+n_after_pun]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_ap_punish.append(var_trial[0])\n",
    "    \n",
    "    # reward\n",
    "    for idx in X:\n",
    "        # only reward trials\n",
    "        if trials_is_reward[idx]==1:\n",
    "            # timepoint for response\n",
    "            resp_timepoint = int((trials_end_time[idx] - new_start[idx]) / sampling_rate)\n",
    "            x = X[idx][resp_timepoint-n_before_rew:resp_timepoint+n_after_rew]\n",
    "            if len(x)>6: # make sure that time window is not too small\n",
    "                covar_matrix = PCA(n_components=n_components)\n",
    "                covar_matrix.fit(x)\n",
    "                variance = covar_matrix.explained_variance_ratio_ #calculate variance ratios\n",
    "                var_trial = np.cumsum(np.round(covar_matrix.explained_variance_ratio_, decimals=3))\n",
    "        variances_ap_reward.append(var_trial[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permutation test: pvalue = 0.037\n",
      "parametric t-test: Ttest_indResult(statistic=-1.9120858534666179, pvalue=0.0560910493073145)\n"
     ]
    }
   ],
   "source": [
    "# change variable1 and variable2 to compute permutation and t-tests between them:\n",
    "# airpuff: variances_ap_start  variances_ap_stim_onset  variances_ap_resp  variances_ap_punish  variances_ap_reward\n",
    "# no airpuff: variances_start  variances_stim_onset  variances_resp  variances_punish  variances_reward\n",
    "variable1 = variances_ap_reward\n",
    "variable2 = variances_reward\n",
    "\n",
    "# Compute the ground truth absolute difference Δτ between τ₁ and τ₂ from your two variables:\n",
    "gT = np.abs(np.average(variable1) - np.average(variable2))\n",
    "\n",
    "# Pool your variables into one single distribution:\n",
    "pV = list(variable1) + list(variable2)\n",
    "\n",
    "# Randomly sample (also called bootstrapping) without replacement two distributions \n",
    "# with the size equal to the original distributions from this pooled distribution \n",
    "# to compute the absolute difference RΔτ of your metric between your two permuted samples:\n",
    "# and repeat this p times:\n",
    "\n",
    "# Copy pooled distribution:\n",
    "pS = copy.copy(pV)\n",
    "# Initialize permutation:\n",
    "pD = []\n",
    "# Define p (number of permutations):\n",
    "p=1000\n",
    "# Permutation loop:\n",
    "for i in range(0,p):\n",
    "  # Shuffle the data:\n",
    "    random.shuffle(pS)\n",
    "    # Compute permuted absolute difference of your two sampled distributions and store it in pD:\n",
    "    pD.append(np.abs(np.average(pS[0:int(len(pS)/2)]) - np.average(pS[int(len(pS)/2):])))\n",
    "    \n",
    "# Finally, the proportion of permuted differences higher than your ground truth difference \n",
    "# is your significance value:\n",
    "p_val = len(np.where(pD>=gT)[0])/p\n",
    "print(\"permutation test: pvalue =\", p_val)\n",
    "# Compare with parametric t-test:\n",
    "print(\"parametric t-test:\", scipy.stats.stats.ttest_ind(variable1, variable2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
